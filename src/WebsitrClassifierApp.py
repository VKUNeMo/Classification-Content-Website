import streamlit as st
import os 
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.ContentExtractor import ContentExtractor
from src.TextSummarise import TextSummarizer
from src.WebsiteClassifier import WebsiteClassifier
import pandas as pd
class WebsiteClassifierApp:
    """L·ªõp ·ª©ng d·ª•ng Streamlit ƒë·ªÉ ph√¢n lo·∫°i website"""
    
    def __init__(self):
        """Kh·ªüi t·∫°o ·ª©ng d·ª•ng"""
        self.content_extractor = ContentExtractor()
        self.summarizer = None
        self.classifier = None
        
        # Thi·∫øt l·∫≠p trang Streamlit
        st.set_page_config(
            page_title="Website Classifier",
            page_icon="üåê",
            layout="wide"
        )
        
    def load_models(self):
        """T·∫£i c√°c m√¥ h√¨nh khi ƒë∆∞·ª£c y√™u c·∫ßu"""
        with st.spinner("ƒêang t·∫£i m√¥ h√¨nh t√≥m t·∫Øt..."):
            self.summarizer = TextSummarizer()
        
        with st.spinner("ƒêang t·∫£i m√¥ h√¨nh ph√¢n lo·∫°i..."):
            self.classifier = WebsiteClassifier()
    
    def run(self):
        """Ch·∫°y ·ª©ng d·ª•ng Streamlit"""
        st.title("Website Classifier üåê")
        st.write("·ª®ng d·ª•ng ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung")
        
        # T·∫°o c√°c tab cho ·ª©ng d·ª•ng
        tab1, tab2 = st.tabs(["Ph√¢n lo·∫°i URL", "Th√¥ng tin"])
        
        with tab1:
            url = st.text_input("Nh·∫≠p ƒë·ªãa ch·ªâ URL website c·∫ßn ph√¢n lo·∫°i:")
            
            col1, col2 = st.columns(2)
            with col1:
                summarize = st.checkbox("T√≥m t·∫Øt n·ªôi dung tr∆∞·ªõc khi ph√¢n lo·∫°i", value=True)
            with col2:
                show_content = st.checkbox("Hi·ªÉn th·ªã n·ªôi dung website", value=False)
            
            if st.button("Ph√¢n lo·∫°i"):
                if not url:
                    st.error("Vui l√≤ng nh·∫≠p URL website")
                else:
                    if not self.summarizer or not self.classifier:
                        self.load_models()
                    
                    # Tr√≠ch xu·∫•t n·ªôi dung
                    with st.spinner("ƒêang tr√≠ch xu·∫•t n·ªôi dung t·ª´ website..."):
                        content = self.content_extractor.extract_content(url)
                    
                    if content.startswith("L·ªói"):
                        st.error(content)
                    else:
                        # T√≥m t·∫Øt n·ªôi dung n·∫øu c·∫ßn
                        if summarize:
                            with st.spinner("ƒêang t√≥m t·∫Øt n·ªôi dung..."):
                                summary = self.summarizer.summarize(content)
                                st.success("ƒê√£ t√≥m t·∫Øt n·ªôi dung th√†nh c√¥ng!")
                                
                                if show_content:
                                    st.subheader("N·ªôi dung t√≥m t·∫Øt:")
                                    st.write(summary)
                            
                            # Ph√¢n lo·∫°i n·ªôi dung t√≥m t·∫Øt
                            with st.spinner("ƒêang ph√¢n lo·∫°i website..."):
                                classification_results = self.classifier.classify(summary)
                        else:
                            # Ph√¢n lo·∫°i n·ªôi dung g·ªëc
                            with st.spinner("ƒêang ph√¢n lo·∫°i website..."):
                                classification_results = self.classifier.classify(content)
                                
                                if show_content:
                                    st.subheader("N·ªôi dung website:")
                                    st.write(content[:2000] + "..." if len(content) > 2000 else content)
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n lo·∫°i
                        st.subheader("K·∫øt qu·∫£ ph√¢n lo·∫°i:")
                        
                        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ th√†nh DataFrame ƒë·ªÉ hi·ªÉn th·ªã
                        results_df = pd.DataFrame({
                            'Lo·∫°i website': list(classification_results.keys()),
                            'ƒêi·ªÉm s·ªë': list(classification_results.values())
                        })
                        
                        results_df = results_df.sort_values('ƒêi·ªÉm s·ªë', ascending=False)
                        
                        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
                        st.bar_chart(results_df.set_index('Lo·∫°i website'))
                        
                        # Hi·ªÉn th·ªã ph√¢n lo·∫°i c√≥ ƒëi·ªÉm cao nh·∫•t
                        st.success(f"Website ƒë∆∞·ª£c ph√¢n lo·∫°i l√†: **{results_df.iloc[0]['Lo·∫°i website']}** v·ªõi ƒëi·ªÉm s·ªë: {results_df.iloc[0]['ƒêi·ªÉm s·ªë']:.4f}")
        
        with tab2:
            st.header("Th√¥ng tin v·ªÅ ·ª©ng d·ª•ng")
            st.write("""
            ·ª®ng d·ª•ng n√†y s·ª≠ d·ª•ng c√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung:
            
            1. **Tr√≠ch xu·∫•t n·ªôi dung**: ·ª®ng d·ª•ng tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n t·ª´ URL website.
            2. **T√≥m t·∫Øt n·ªôi dung**: S·ª≠ d·ª•ng m√¥ h√¨nh BART t·ª´ Facebook ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung website.
            3. **Ph√¢n lo·∫°i website**: S·ª≠ d·ª•ng m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh danh m·ª•c c·ªßa website.
            
            ·ª®ng d·ª•ng ƒë∆∞·ª£c ph√°t tri·ªÉn s·ª≠ d·ª•ng Streamlit v√† Hugging Face Transformers.
            """)
            
            st.subheader("H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
            st.write("""
            1. Nh·∫≠p URL website c·∫ßn ph√¢n lo·∫°i.
            2. Ch·ªçn t√πy ch·ªçn t√≥m t·∫Øt n·ªôi dung n·∫øu mu·ªën.
            3. Nh·∫•n n√∫t "Ph√¢n lo·∫°i" ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh.
            4. Xem k·∫øt qu·∫£ ph√¢n lo·∫°i website.
            """)
