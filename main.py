import streamlit as st
import requests
from bs4 import BeautifulSoup
import re
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
from transformers import AutoModelForSeq2SeqLM
import torch
import pandas as pd

class ContentExtractor:
    """L·ªõp tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL website"""
    
    def __init__(self):
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
    
    def extract_content(self, url):
        """
        Tr√≠ch xu·∫•t n·ªôi dung t·ª´ URL website
        
        Args:
            url (str): URL c·ªßa website c·∫ßn tr√≠ch xu·∫•t
            
        Returns:
            str: N·ªôi dung website sau khi ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        try:
            response = requests.get(url, headers=self.headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Lo·∫°i b·ªè c√°c th·∫ª kh√¥ng li√™n quan
            for tag in soup(['script', 'style', 'header', 'footer', 'nav']):
                tag.decompose()
            
            # L·∫•y vƒÉn b·∫£n t·ª´ th·∫ª body
            text = soup.get_text(separator=' ', strip=True)
            
            # X·ª≠ l√Ω vƒÉn b·∫£n
            text = re.sub(r'\s+', ' ', text)  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
            text = re.sub(r'\n+', ' ', text)  # Lo·∫°i b·ªè xu·ªëng d√≤ng
            
            # Gi·ªõi h·∫°n ƒë·ªô d√†i vƒÉn b·∫£n ƒë·ªÉ tr√°nh l·ªói int too big to convert
            if len(text) > 100000:  # Gi·ªõi h·∫°n ·ªü kho·∫£ng 100k k√Ω t·ª±
                text = text[:100000]
            
            return text
        except Exception as e:
            return f"L·ªói khi tr√≠ch xu·∫•t n·ªôi dung: {str(e)}"


class TextSummarizer:
    """L·ªõp t√≥m t·∫Øt n·ªôi dung vƒÉn b·∫£n s·ª≠ d·ª•ng m√¥ h√¨nh t·ª´ Hugging Face"""
    
    def __init__(self, model_name="facebook/bart-large-cnn"):
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh t√≥m t·∫Øt
        
        Args:
            model_name (str): T√™n m√¥ h√¨nh t√≥m t·∫Øt t·ª´ Hugging Face
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        
    def summarize(self, text, max_length=512, min_length=100):
        """
        T√≥m t·∫Øt vƒÉn b·∫£n
        
        Args:
            text (str): VƒÉn b·∫£n c·∫ßn t√≥m t·∫Øt
            max_length (int): ƒê·ªô d√†i t·ªëi ƒëa c·ªßa vƒÉn b·∫£n t√≥m t·∫Øt
            min_length (int): ƒê·ªô d√†i t·ªëi thi·ªÉu c·ªßa vƒÉn b·∫£n t√≥m t·∫Øt
            
        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c t√≥m t·∫Øt
        """
        try:
            # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·∫ßu v√†o theo model_max_length c·ªßa tokenizer
            max_input_length = self.tokenizer.model_max_length
            if max_input_length > max_length:  # Th√™m gi·ªõi h·∫°n b·ªï sung ƒë·ªÉ tr√°nh l·ªói
                max_input_length = max_length
                
            # Tokenize v·ªõi truncation ƒë·ªÉ ƒë·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° max_input_length
            inputs = self.tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True,
                max_length=max_input_length
            )
            
            inputs = inputs.to(self.device)
            
            # ƒê·∫£m b·∫£o max_length kh√¥ng qu√° l·ªõn
            if max_length > max_length:
                max_length = max_length
                
            summary_ids = self.model.generate(
                inputs["input_ids"],
                max_length=max_length,
                min_length=min_length,
                num_beams=4,
                length_penalty=2.0,
                early_stopping=True
            )
            
            summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            return summary
        except Exception as e:
            st.error(f"L·ªói khi t√≥m t·∫Øt vƒÉn b·∫£n: {str(e)}")
            # Tr·∫£ v·ªÅ m·ªôt ph·∫ßn vƒÉn b·∫£n g·ªëc n·∫øu kh√¥ng th·ªÉ t√≥m t·∫Øt
            return text[:500] + "... (kh√¥ng th·ªÉ t√≥m t·∫Øt ƒë·∫ßy ƒë·ªß)"
        
class WebsiteClassifier:
    """L·ªõp ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung"""
    
    def __init__(self, model_name="distilbert-base-uncased-finetuned-sst-2-english"):
        """
        Kh·ªüi t·∫°o m√¥ h√¨nh ph√¢n lo·∫°i
        
        Args:
            model_name (str): T√™n m√¥ h√¨nh ph√¢n lo·∫°i t·ª´ Hugging Face
        """
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.model.to(self.device)
        
        # ƒê·ªãnh nghƒ©a c√°c nh√£n ph√¢n lo·∫°i (thay ƒë·ªïi theo m√¥ h√¨nh c·ª• th·ªÉ)
        self.labels = ["Tin t·ª©c", "Th∆∞∆°ng m·∫°i ƒëi·ªán t·ª≠", "Gi√°o d·ª•c", "Gi·∫£i tr√≠", "C√¥ng ngh·ªá"]
        
    def classify(self, text):
        """
        Ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung
        
        Args:
            text (str): N·ªôi dung website c·∫ßn ph√¢n lo·∫°i
            
        Returns:
            dict: K·∫øt qu·∫£ ph√¢n lo·∫°i v·ªõi ƒëi·ªÉm s·ªë cho t·ª´ng nh√£n
        """
        # T·∫°o pipeline ph√¢n lo·∫°i
        classifier = pipeline(
            "text-classification", 
            model=self.model, 
            tokenizer=self.tokenizer,
            device=0 if self.device == "cuda" else -1
        )
        
        # C·∫Øt vƒÉn b·∫£n th√†nh c√°c ƒëo·∫°n ƒë·ªÉ tr√°nh v∆∞·ª£t qu√° gi·ªõi h·∫°n ƒë·ªô d√†i
        max_length = self.tokenizer.model_max_length
        chunks = [text[i:i+max_length] for i in range(0, len(text), max_length)]
        
        # Ph√¢n lo·∫°i t·ª´ng ƒëo·∫°n
        results = []
        for chunk in chunks[:5]:  # Gi·ªõi h·∫°n s·ªë ƒëo·∫°n ƒë·ªÉ tr√°nh t·ªën th·ªùi gian
            result = classifier(chunk)
            results.append(result[0])
        
        # Tr·∫£ v·ªÅ k·∫øt h·ª£p c√°c k·∫øt qu·∫£ (v√≠ d·ª• l·∫•y ph√¢n lo·∫°i c√≥ ƒëi·ªÉm cao nh·∫•t)
        if results:
            # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ t√πy theo m√¥ h√¨nh
            scores = {}
            for i, label in enumerate(self.labels):
                scores[label] = sum(result["score"] for result in results if result["label"] == f"LABEL_{i}") / len(results)
            
            return scores
        else:
            return {label: 0.0 for label in self.labels}


class WebsiteClassifierApp:
    """L·ªõp ·ª©ng d·ª•ng Streamlit ƒë·ªÉ ph√¢n lo·∫°i website"""
    
    def __init__(self):
        """Kh·ªüi t·∫°o ·ª©ng d·ª•ng"""
        self.content_extractor = ContentExtractor()
        self.summarizer = None
        self.classifier = None
        
        # Thi·∫øt l·∫≠p trang Streamlit
        st.set_page_config(
            page_title="Website Classifier",
            page_icon="üåê",
            layout="wide"
        )
        
    def load_models(self):
        """T·∫£i c√°c m√¥ h√¨nh khi ƒë∆∞·ª£c y√™u c·∫ßu"""
        with st.spinner("ƒêang t·∫£i m√¥ h√¨nh t√≥m t·∫Øt..."):
            self.summarizer = TextSummarizer()
        
        with st.spinner("ƒêang t·∫£i m√¥ h√¨nh ph√¢n lo·∫°i..."):
            self.classifier = WebsiteClassifier()
    
    def run(self):
        """Ch·∫°y ·ª©ng d·ª•ng Streamlit"""
        st.title("Website Classifier üåê")
        st.write("·ª®ng d·ª•ng ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung")
        
        # T·∫°o c√°c tab cho ·ª©ng d·ª•ng
        tab1, tab2 = st.tabs(["Ph√¢n lo·∫°i URL", "Th√¥ng tin"])
        
        with tab1:
            url = st.text_input("Nh·∫≠p ƒë·ªãa ch·ªâ URL website c·∫ßn ph√¢n lo·∫°i:")
            
            col1, col2 = st.columns(2)
            with col1:
                summarize = st.checkbox("T√≥m t·∫Øt n·ªôi dung tr∆∞·ªõc khi ph√¢n lo·∫°i", value=True)
            with col2:
                show_content = st.checkbox("Hi·ªÉn th·ªã n·ªôi dung website", value=False)
            
            if st.button("Ph√¢n lo·∫°i"):
                if not url:
                    st.error("Vui l√≤ng nh·∫≠p URL website")
                else:
                    if not self.summarizer or not self.classifier:
                        self.load_models()
                    
                    # Tr√≠ch xu·∫•t n·ªôi dung
                    with st.spinner("ƒêang tr√≠ch xu·∫•t n·ªôi dung t·ª´ website..."):
                        content = self.content_extractor.extract_content(url)
                    
                    if content.startswith("L·ªói"):
                        st.error(content)
                    else:
                        # T√≥m t·∫Øt n·ªôi dung n·∫øu c·∫ßn
                        if summarize:
                            with st.spinner("ƒêang t√≥m t·∫Øt n·ªôi dung..."):
                                summary = self.summarizer.summarize(content)
                                st.success("ƒê√£ t√≥m t·∫Øt n·ªôi dung th√†nh c√¥ng!")
                                
                                if show_content:
                                    st.subheader("N·ªôi dung t√≥m t·∫Øt:")
                                    st.write(summary)
                            
                            # Ph√¢n lo·∫°i n·ªôi dung t√≥m t·∫Øt
                            with st.spinner("ƒêang ph√¢n lo·∫°i website..."):
                                classification_results = self.classifier.classify(summary)
                        else:
                            # Ph√¢n lo·∫°i n·ªôi dung g·ªëc
                            with st.spinner("ƒêang ph√¢n lo·∫°i website..."):
                                classification_results = self.classifier.classify(content)
                                
                                if show_content:
                                    st.subheader("N·ªôi dung website:")
                                    st.write(content[:2000] + "..." if len(content) > 2000 else content)
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ ph√¢n lo·∫°i
                        st.subheader("K·∫øt qu·∫£ ph√¢n lo·∫°i:")
                        
                        # Chuy·ªÉn ƒë·ªïi k·∫øt qu·∫£ th√†nh DataFrame ƒë·ªÉ hi·ªÉn th·ªã
                        results_df = pd.DataFrame({
                            'Lo·∫°i website': list(classification_results.keys()),
                            'ƒêi·ªÉm s·ªë': list(classification_results.values())
                        })
                        
                        results_df = results_df.sort_values('ƒêi·ªÉm s·ªë', ascending=False)
                        
                        # Hi·ªÉn th·ªã bi·ªÉu ƒë·ªì
                        st.bar_chart(results_df.set_index('Lo·∫°i website'))
                        
                        # Hi·ªÉn th·ªã ph√¢n lo·∫°i c√≥ ƒëi·ªÉm cao nh·∫•t
                        st.success(f"Website ƒë∆∞·ª£c ph√¢n lo·∫°i l√†: **{results_df.iloc[0]['Lo·∫°i website']}** v·ªõi ƒëi·ªÉm s·ªë: {results_df.iloc[0]['ƒêi·ªÉm s·ªë']:.4f}")
        
        with tab2:
            st.header("Th√¥ng tin v·ªÅ ·ª©ng d·ª•ng")
            st.write("""
            ·ª®ng d·ª•ng n√†y s·ª≠ d·ª•ng c√°c m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ ph√¢n lo·∫°i website d·ª±a tr√™n n·ªôi dung:
            
            1. **Tr√≠ch xu·∫•t n·ªôi dung**: ·ª®ng d·ª•ng tr√≠ch xu·∫•t n·ªôi dung vƒÉn b·∫£n t·ª´ URL website.
            2. **T√≥m t·∫Øt n·ªôi dung**: S·ª≠ d·ª•ng m√¥ h√¨nh BART t·ª´ Facebook ƒë·ªÉ t√≥m t·∫Øt n·ªôi dung website.
            3. **Ph√¢n lo·∫°i website**: S·ª≠ d·ª•ng m√¥ h√¨nh ph√¢n lo·∫°i vƒÉn b·∫£n ƒë·ªÉ x√°c ƒë·ªãnh danh m·ª•c c·ªßa website.
            
            ·ª®ng d·ª•ng ƒë∆∞·ª£c ph√°t tri·ªÉn s·ª≠ d·ª•ng Streamlit v√† Hugging Face Transformers.
            """)
            
            st.subheader("H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng")
            st.write("""
            1. Nh·∫≠p URL website c·∫ßn ph√¢n lo·∫°i.
            2. Ch·ªçn t√πy ch·ªçn t√≥m t·∫Øt n·ªôi dung n·∫øu mu·ªën.
            3. Nh·∫•n n√∫t "Ph√¢n lo·∫°i" ƒë·ªÉ b·∫Øt ƒë·∫ßu qu√° tr√¨nh.
            4. Xem k·∫øt qu·∫£ ph√¢n lo·∫°i website.
            """)


if __name__ == "__main__":
    app = WebsiteClassifierApp()
    app.run()